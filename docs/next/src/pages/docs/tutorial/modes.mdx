import PyObject from 'components/PyObject';

# Pipeline modes

By attaching different sets of resources with the same APIs to different
modes, we can support running pipelines -- with unchanged business
logic -- in different environments. So you might have a "unittest"
mode that runs against an in-memory SQLite database, a "dev" mode that
runs against Postgres, and a "prod" mode that runs against Snowflake.

Separating the resource definition from the business logic makes
pipelines testable. As long as the APIs of the resources agree, and the
fundamental operations they expose are tested in each environment, we
can test business logic independent of environments that may be very
costly or difficult to test against.

```python literalinclude
file:/intro_tutorial/modes.py
caption:modes.py
lineno-start:75
lines:75-84
```

Even if you're not familiar with SQLAlchemy, it's enough to note that
this is a very different implementation of the `warehouse` resource. To
make this implementation available to Dagster, we attach it to a <PyObject module="dagster" object="ModeDefinition" />.

```python literalinclude
file:/intro_tutorial/modes.py
caption:modes.py
lineno-start:127
lines:127-142
```

Each of the ways we can invoke a Dagster pipeline lets us select which
mode we'd like to run it in.

From the command line, we can set `-d` or `--mode` and select the name
of the mode:

```bash
$ dagster pipeline execute -f modes.py -n modes_pipeline -d dev
```

Or, from the Python API:

```python literalinclude
file:/intro_tutorial/modes.py
caption:modes.py
lineno-start:152
lines:152-156
emphasize-lines:4
```

And in Dagit, we can use the "Mode" selector to pick the mode in which
we'd like to execute.

![modes.png](/assets/images/tutorial/modes.png)

The config editor is Dagit is mode-aware, so when you switch modes and
introduce a resource that requires additional config, the editor will
prompt you.
