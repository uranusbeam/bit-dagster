import PyObject from 'components/PyObject';

# Materializations

Steps in a data pipeline often produce persistent artifacts, for instance,
graphs or tables describing the result of some computation. Typically these
artifacts are saved to disk (or to cloud storage) with a [name](https://xkcd.com/1459/) that
has something to do with their origin. But it can be hard to organize and cross-reference
artifacts produced by many different runs of a pipeline, or to identify all of the files that
might have been created by some pipeline's logic.

Dagster solids can describe their persistent artifacts to the system by
yielding <PyObject module="dagster" object="Materialization" /> events. Like <PyObject module="dagster" object="TypeCheck" /> and <PyObject module="dagster" object="ExpectationResult" />,
materializations are side-channels for metadata -- they don't get passed
to downstream solids and they aren't used to define the data dependencies that
structure a pipeline's DAG.

Suppose that we rewrite our `sort_calories` solid so that it saves
the newly sorted data frame to disk.

```python literalinclude
file:/intro_tutorial/materializations.py
caption:materializations.py
lines:23-46
```

We've taken the basic precaution of ensuring that the saved csv file has a
different filename for each run of the pipeline. But there's no way for Dagit
to know about this persistent artifact. So we'll add the following lines:

```python literalinclude
file:/intro_tutorial/materializations.py
caption:materializations.py
lineno-start:23
lines:23-56
emphasize-lines:24-33
```

Note that we've had to add the last line, yielding an <PyObject module="dagster" object="Output" />.
Until now, all of our solids have relied on Dagster's implicit conversion
of the return value of a solid's compute function into its output. When we explicitly
yield other types of events from solid logic, we need to also explicitly yield
the output so that the framework can recognize them.

Now, if we run this pipeline in Dagit:

![materializations.png](/assets/images/tutorial/materializations.png)

## Configurably materializing custom data types

Data types can also be configured so that outputs materialize themselves,
obviating the need to explicitly yield a <PyObject module="dagster" object="Materialization" /> from solid logic.
Dagster calls this facility the <PyObject module="dagster" object="output_materialization_config" displayText="@output_materialization_config" />.

Suppose we would like to be able to configure outputs of our toy custom type,
the `SimpleDataFrame`, to be automatically materialized to disk as
both as a pickle and as a .csv. (This is a reasonable idea, since .csv files
are human-readable and manipulable by a wide variety of third party tools,
while pickle is a binary format.)

```python literalinclude
file:/intro_tutorial/output_materialization.py
caption:output_materialization.py
lineno-start:28
lines:28-61
```

We set the output materialization config on the type:

```python literalinclude
file:/intro_tutorial/output_materialization.py
caption:output_materialization.py
lineno-start:64
lines:64-71
emphasize-lines:5
```

Now we can tell Dagster to materialize intermediate outputs of this type by
providing config:

```YAML literalinclude
file:/intro_tutorial/output_materialization.yaml
caption:output_materialization.yaml
emphasize-lines:6-10
```

When we run this pipeline, we'll see that materializations are yielded (and
visible in the structured logs in Dagit), and that files are created on disk
(with the semicolon separator we specified).

![output_materializations.png](/assets/images/tutorial/output_materializations.png)
